<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>CS 184 Rasterizer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2022</h1>
<h1 align="middle">Project 3</h1>
<h2 align="middle">Jasmine Wang, 3034027325</h2>
<h2 align="middle">Victor Ho, 3033712816</h2>
<h2 align="middle">website link:</h2>
 <a href="https://cal-cs184-student.github.io/sp22-project-webpages-jwang99/proj2/index.html">https://cal-cs184-student.github.io/sp22-project-webpages-jwang99/proj2/index.html</a>

<br><br>

<h1 align="middle">Assignment 3: PathTracer</h1>
    <h2 align="middle">Your Name</h2>

    <div class="padded">
        <p>In this Project, we created the foundation for shading and coloring objects in a lit scene through ray tracing. We first established the basic tools such as detecting intersection with primitives, mapping world points to pixel locations, and building a BVH datastructure that makes intersection detection more efficient. Then we implemented progressively more complex illumination schemes starting with direct (zero and single bounce) lighting and eventually incorporating indirect (atleast one bounce) illumination, which used the primitives previously established to determine the proper illumination for pixels representing different points in a scene. Finally, we added an algorithm to speedup the rendering process of these scenes by establishing a "sampled enough" heuristic through adaptive sampling</p>

    <h2 align="middle">Part 1: Ray Generation and Scene Intersection: </h2>

    	<h3>Ray generation and primitve intersections</h3>
    	<p>Ray generation and primitive intersection is the first step in the rendering pipeline. In order for a camera to be able to render an image of what a 3D scene looks like, it must determine what should be represented in each pixel of the image. To do so, a transfomration is first established between the camera's image (normalized image coordinates) and the camera's virtual sensor (world frame coordinates), a plane orthogonal to the camera's direction. Then, we create a ray in the the world frame, from the cameras location, to the point in the world frame which corresponds to the location of that pixel. This ray represents the "viewing direction" for the objects that should be seen in the pixel. This process is known as ray generation.</p>
    	<p>Once the appropriate ray has been generated, to properly determine what color or object should appear in the image at each image coordinate, just follow along the direction of the ray until it intersects an object primitive (triangle, sphere, etc) that is used to build objects in the scene. Once an intersection is detected, the color of the image coordinate, should be dependent on the illuminance of the object in reverse direction of the ray.</p>

    	<h3>Triangle Intersection Algorithm</h3>
    	<p>My triangle-ray intersection algorithm is primarily built off of the Moller Trumbore Algorithm. Using the Moller Trumbore Algorithm on the triangle-ray pair that we are finding an intersection for, we get the values t (time of intersection), b1, and b2 (two of the 3 barycentric coordinates for the triangle). We first inspect the t value to make sure that it is within the lifespan of the ray (between t_min and t_max), since rays are considered as finite segments of the entire ray. Then we check the barycentric coordinates b0 = 1-b1-b2, b1, and b2 returned by the Moller Trumbore Algorithm to make sure that they represent a point that is actually INSIDE the triangle, meaning that all barycentric coordinates are between 0 and 1. If both conditions are true, the intersection is within the lifespan of the ray and the intersection point is inside the triangle, then there is an intersection between the given ray and triangle. </p>

    	<h3>Example images with normal shading for a few small .dae files</h3>

    	<div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part1/CBEmpty.png" width="480px" />
                    <figcaption align="middle">CBEmpty.dae renderred with normal shading</figcaption>
                </tr>
            </table>
        </div>

        <br><br><br>
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part1/CBSpheres.png" width="480px" />
                    <figcaption align="middle">CBSpheres_lambertian.dae renderred with normal shading</figcaption>
                </tr>
            </table>
        </div>

    <br><br><br><br>
    <h2 align="middle">Part 2: Bounding Volume Hierarchy: </h2>
    	<h3>BVH Construction Algorithm</h3>
    	<p>The Construction of a Bounding Volume Hierarchy is a recursive process, where the BVH itself is a tree structure of BVH Nodes. When given a list of primitives that we are trying to segment into separate bounding volumes, if the number of primitives in this list is small enough to be contained in a single leaf node, then allocate a leaf node and the process is finished. Otherwise, if the number of primitives is too large to be contained in a single leaf node, we need to divide the primitives in half and create a sub-BVH tree for each half. The way that the primitives are allocated into 2 groups is: determine which axis (x, y, z) has the widest range in which primitives exist, and find the midpoint of this range. All primitives whose centroid is on one side of this midpoint gets put into one group, and all primitives whose centroid is on the other side is put into the other group. If there arises a situation in which one of these 2 groups has no primitives, forcibly move one primitve from the other group into this group. </p>
    	<h3>Example images with normal shading for a few large .dae files using BVH acceleration</h3>

    	<div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part2/banana.png" width="480px" />
                    <figcaption align="middle">banana.dae renderred with normal shading</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part2/cow.png" width="480px" />
                    <figcaption align="middle">cow.dae renderred with normal shading</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part2/maxplanck.png" width="480px" />
                    <figcaption align="middle">maxplanck.dae renderred with normal shading</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part2/CBlucy.png" width="480px" />
                    <figcaption align="middle">CBlucy.dae renderred with normal shading</figcaption>
                </tr>
            </table>
        </div>

    	<h3>Render Time Comparison on Complex Geometries With and Without BVH acceleration</h3>
    	<p>Rendering time for complex geometries decreased SUBSTANTIALLY when BVH acceleration was used. This is because we no longer have to check for intersection between each ray and EVERY SINGLE primitive in the scene we are trying to render. For complex geometries like maxplanck.dae and CBlucy.dae, these contain over 50,000 and 130,000 primitives respectively. To do a linear search for intersection with each primitive for each ray would take incredibly long. When BVH acceleration is used, not only can we first check for possible intersection with a collection of primitives by using BBox-es that envelop multiple primitives, but due to the tree structure of the BVH, we can cut down the search space of our intersetion in half with every recursive call leading to significantly improved performance. These performance improvements can be seen in the render time comparisons of the following scenes with and without BVH acceleration. Notice that CBlucy.dae was not attempted without BVH acceleration because maxplanck already couldn't finish renderring within 2 minutes despite being able to finish in .06 seconds with BVH acceleration</p>

    	<div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part2/rendertime_noBVH/banana.png" width="480px" />
                    <figcaption align="middle">banana.dae renderred in 21 seconds with normal shading WITHOUT BVH acceleration</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part2/rendertime_noBVH/cow.png" width="480px" />
                    <figcaption align="middle">cow.dae renderred in 34 seconds with normal shading WITHOUT BVH acceleration</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part2/rendertime_noBVH/maxplanck.png" width="480px" />
                    <figcaption align="middle">the progress made in 2 minutes to render maxplanck.dae with normal shading WITHOUT BVH acceleration</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part2/rendertime_withBVH/banana.png" width="480px" />
                    <figcaption align="middle">banana.dae renderred in 0.14 seconds with normal shading WITH BVH acceleration</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part2/rendertime_withBVH/cow.png" width="480px" />
                    <figcaption align="middle">cow.dae renderred in 0.04 seconds with normal shading WITH BVH acceleration</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part2/rendertime_withBVH/maxplanck.png" width="480px" />
                    <figcaption align="middle">maxplanck.dae renderred in 0.06 seconds with normal shading WITH BVH acceleration</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part2/rendertime_withBVH/CBlucy.png" width="480px" />
                    <figcaption align="middle">CBlucy.dae renderred in 0.05 seconds with normal shading WITH BVH acceleration</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

    <br><br><br><br>
    <h2 align="middle">Part 3: Direct Illumination: </h2>
    	<h3>Direct Lighting Implementation Walkthrough</h3>
    	<h4>Uniform Hemisphere Sampling</h4>
    	<p>To estimate the direct lighting at a given point, the uniform hemisphere sampling method involves taking a specified number of uniformly random sample directions within the hemisphere about that point. Then for each sample direction, trace a ray out in that direction (in the world frame) and determine if the ray intersects any other primitives in the scene within the lifespan of the ray. If there isn't an intersection, then this direction contributes nothing to the direct lighting at the given point. If there is an intersection, however, then we take into consideration the irradiance coming from the object the ray intersected. These irradiance values are included in one of the terms in the Monte Carlo estimation of the reflectance equation, which eventually represents the amount of outgoing light.</p>

    	<h4>Importance Sampling Lights</h4>
    	<p>To estimate the direct lighting at a given point, the importance sampling of lights operates differently from the Uniform Hemisphere Sampling method. Rather than taking uniform random samples in a hemisphere, this method iterate thorugh every light source in the scene. Depending on the type of light source, the method will either trace a ray (in the world frame) to that light source (point light source) or will trace rays to multiple points on the light sampled randomly (area light source). For each of these rays, if there is an intersection before the ray reaches the light source, then this sample on the light source contributes nothing to the direct lighting at the given point. If there is NOT an intersection with another primitive before the ray reaches the light source, then we take into consideration the light emitted by the light source at the point we sampled and include it in the total outgoing light from our original given poitn. This is somewhat opposite from the uniform hemisphere sampling method in which illuminance from a direction contributes to the outgoing light if there was an intersection in that direction. For area lights, the contributions of the rays for all samples on that area light are averaged to find the average contribution. The outgoing light from a given point due to all light sources are accumulated to equal the outgoing light from the given point</p>

    	<h3>Some Images Renderred with Both Implementations of Direct Lighting Function</h3>

    	<div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part3/spheres_hemisphere.png" width="480px" />
                    <figcaption align="middle">CBSpheres_lambertian.dae renderred with the uniform hemisphere sampling implementation of the direct lighting function</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part3/spheres_light.png" width="480px" />
                    <figcaption align="middle">CBSpheres_lambertian.dae renderred with the importance sampling lights implementation of the direct lighting function, demonstrating dramatically reduced noise in the image</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part3/bunny_hemisphere.png" width="480px" />
                    <figcaption align="middle">CBBunny.dae renderred with the uniform hemisphere sampling implementation of the direct lighting function</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part3/bunny_light.png" width="480px" />
                    <figcaption align="middle">CBBunny.dae renderred with the importance sampling lights implementation of the direct lighting function, demonstrating dramatically reduced noise in the image</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part3/bench_hemisphere.png" width="480px" />
                    <figcaption align="middle">bench.dae renderred with the uniform hemisphere sampling implementation of the direct lighting function. This scene is black because there aren't other primitives in the scene to act as light sources and it's unlikely that the randomly sampled rays directions are pointing in the right direction to reach the light source</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part3/bench_light.png" width="480px" />
                    <figcaption align="middle">bench.dae renderred with the importance sampling lights implementation of the direct lighting function. The bench actually appears in this render becuase we explicitly traced rays to all light sources in the scene</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <h3>Comparison of Noise Levels in Soft Shadows in CBBunny.dae with varying numbers of light rays, when renderred with 1 sample per pixel and the importance sampling implementation of the direct lighting function</h3>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part3/bunny_1.png" width="480px" />
                    <figcaption align="middle">CBBunny.dae renderred with 1 light ray, showing a significant amount of noise in soft shadows. The edges of the shadow of the bunny as well as in the corners of the room don't fade smoothly, instead are seen as wide pools of heavy shadow.</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part3/bunny_4.png" width="480px" />
                    <figcaption align="middle">CBBunny.dae renderred with 4 light ray, shows significant improvement in the noise in the soft shadows compared to when renderred with 1 light ray</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part3/bunny_16.png" width="480px" />
                    <figcaption align="middle">CBBunny.dae renderred with 16 light ray shows lower levels of noise in the edges of shadows shows significantly reduced intensity and much better diffused as seen from the shadows' edges being more grey than black now</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part3/bunny_64.png" width="480px" />
                    <figcaption align="middle">CBBunny.dae renderred with 64 light ray, showing significantly less noise in soft shadows, as witnessed in the edges of the bunny's shadow and the corners of the room, the shadow  diminishes gracefully and smoothly in soft shadow regions.</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>


        <h3>Comparison of importance and uniform hemisphere light sampling</h3>

        <p>As we can see above, the results differ drastically for direct lighting by importance sampling and for direct lighting with uniform hemisphere sampling. The first major difference is that for our point light source, the bench, the bench is not rendered at all for the hemisphere sampling. This is because it is unlikely that the exact direction towards the point light is sampled with this mode,
      so our algorithm simply doesn't find any sources of illuminance in uniformly sampling across the hemisphere for some point/pixel. In comparison, in importance sampling we only sample the valid directions from the bench to the point light source, allowing us to actually detect the point light source. For the other two images, the spheres and the bunny, we can see that the importance sampling yields a much less noisy image. While the uniform
      hemisphere sampling results in a lot of graininess due to a high variance in samples across the hemisphere and missing the light source in the vast majority of the samples, our importance sampled images are very smooth because the samples are dedicated towards directions that actually contribute or are likely to contribute to our lighting.</p>

  
  
  
  
  
   <h2 align="middle">Part 5: Adaptive Sampling: </h2>
    	<h3>Adaptive Sampling Implementation</h3>
      <p>To implement adaptive sampling, we used the algorithm described in the spec. Instead of the previous implementation of a fixed n samples per pixel, we terminate sampling when a function of the variance of the samples, number of samples, and mean of the samples has been fulfilled. Once our samples have converged to the desired degree, we stop sampling.</p>
      <p>We define two more variables, s1 and s2, to help us check after each sample if we have converged enough to stop sampling. We store in s1 a running sum of all of the illuminance values of the samples, and we store in s2 a running sum of the square of the illuminance values of the samples.
        From these two values, we can calculate our mean and variance of our samples so far: the mean is defined as s1 divided by the number of samples, and the variance is defined as sqrt((1/(n-1)) * (s2 - s1**2/n)), where n is our number of samples. From these statistics, we can calculate our heuristic value I, which is defined as 1.96 * variance / sqrt(numsamples).
        We want to terminate sampling when I <= maxtolerance * mean, where maxtolerance is a hyperparamter (0.05 by default.) For efficiency, we only calculate the heuristic once every numsamplesperbatch samples, where numsamplesberpatch is a hyperparameter (default 32), and terminate sampling once this check passes or we reach the maximum allowed number of samples.</p>
    	<h3>Example images showing the image rendered using adaptive sampling as well as the sample rate image, where red signifies the highest sampling rate and blue signifies the lowest sampling rate.</h3>

    	<div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part5/bunny.png" width="480px" />
                    <figcaption align="middle">bunny rendered with adaptive sampling</figcaption>
                </tr>
                <tr>
                    <td align="middle">
                    <img src="images/part5/bunny_rate.png" width="480px" />
                    <figcaption align="middle">sampling rates of bunny sampled with adaptive sampling</figcaption>
                </tr>
            </table>
        </div>


        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/part5/dragon_adaptive.png" width="480px" />
                    <figcaption align="middle">dragon rendered with adaptive sampling<</figcaption>
                </tr>
                <tr>
                    <td align="middle">
                    <img src="images/part5/dragon_adaptive_rate.png" width="480px" />
                    <figcaption align="middle">sampling rates of dragon sampled with adaptive sampling<</figcaption>
                </tr>
            </table>
        </div>
        <br><br><br>
       
        
              <p>We see here that the shadows and darker areas have significantly higher sampling rates than those in direct light. This is because for these areas, the variance in the samples was much higher. This is also why these areas were much noisier without adaptive sampling. Thus, using this heuristic, we naturally sample noisier areas more</p>


    	


    <h2 align="middle">A Few Notes On Webpages</h2>
        <p>Here are a few problems students have encountered in the past. You will probably encounter these problems at some point, so don't wait until right before the deadline to check that everything is working. Test your website on the instructional machines early!</p>
        <ul>
        <li>Your main report page should be called index.html.</li>
        <li>Be sure to include and turn in all of the other files (such as images) that are linked in your report!</li>
        <li>Use only <em>relative</em> paths to files, such as <pre>"./images/image.jpg"</pre>
        Do <em>NOT</em> use absolute paths, such as <pre>"/Users/student/Desktop/image.jpg"</pre></li>
        <li>Pay close attention to your filename extensions. Remember that on UNIX systems (such as the instructional machines), capitalization matters. <pre>.png != .jpeg != .jpg != .JPG</pre>
        <li>Be sure to adjust the permissions on your files so that they are world readable. For more information on this please see this tutorial: <a href="http://www.grymoire.com/Unix/Permissions.html">http://www.grymoire.com/Unix/Permissions.html</a></li>
        <li>And again, test your website on the instructional machines early!</li>
</div>
</body>
</html>




